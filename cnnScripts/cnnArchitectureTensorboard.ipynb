{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "84fgVKnnneBH"
      },
      "source": [
        "To run with tensorboard. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classifying Alzheimer's Using CyTOF Data and Deep Learning. \n",
        "### 31/05/2023 ####\n",
        "Building from scratch, attempting to follow the structure of DeepLearningCyTOF (Hu et al. (2020)) but implement it in PyTorch rather than Keras. Also want to implement (optional) k-fold cross-validation. \n",
        "At the moment there are two code blocks to perform training/plotting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYDGEWgznPXP",
        "outputId": "d9be7998-f6b8-4a55-c1a4-444fef0d7890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qypBzOLPoU_c"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijP2u4gxA7fD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 1: Import functions #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO3DlSTtA99K"
      },
      "outputs": [],
      "source": [
        "## From Hu et al. (2020)\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed; seed(111)\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_ind\n",
        "from IPython.display import Image  \n",
        "\n",
        "## 0. Import \n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC4dBKDCBAMU"
      },
      "outputs": [],
      "source": [
        "class AdDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        ## Build a list of tuples\n",
        "        # Here x is our .csv files (i.e. CyTOF data)\n",
        "        # Here y is our output (0 for a control and 1 for a AD patient)\n",
        "        \n",
        "        self.data_dir = data_dir\n",
        "        self.x = os.listdir(data_dir)\n",
        "        self.y= []  # Initialize an empty list to store class labels\n",
        "\n",
        "        for file_name in self.x:\n",
        "            # Extract the letter preceding \".csv\" in the file name\n",
        "            y_label = file_name.split(\".\")[0][-1]\n",
        "            # Check if the class label is \"C\" and assign 0, else assign 1\n",
        "            if y_label == \"C\":\n",
        "                self.y.append(0)\n",
        "                #print(0)\n",
        "            else:\n",
        "                self.y.append(1)\n",
        "                #print(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        ## Size of whole data set\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ## For loading data on demand, rather than loaded in __init__ step, to increase memory inefficiency. \n",
        "\n",
        "        file_path = os.path.join(self.data_dir, self.x[idx])\n",
        "        data = pd.read_csv(file_path, sep=\"\\t\", header=None).values\n",
        "        data = torch.from_numpy(data)\n",
        "        label = self.y[idx]  # Get the class label for the corresponding file WATCH OUT FOR FLOAT --> MAY CAUSE ERRORS BECAUSE DATA NOT IN SAME DTYPE AS CLASS_LABEL\n",
        "        return data, label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 2: Load data #####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HX5rIey6BCaf"
      },
      "outputs": [],
      "source": [
        "data_dir_train = \"/content/drive/MyDrive/colabData/st1/train\" # 290\n",
        "train_dataset = AdDataset(data_dir_train)\n",
        "\n",
        "data_dir_val = \"/content/drive/MyDrive/colabData/st1/validate\"\n",
        "val_dataset = AdDataset(data_dir_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=4, shuffle=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Functions to evaluate model performance #####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt1kTgghBVsv"
      },
      "outputs": [],
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "    return F2.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plKfD9dSBXRP"
      },
      "outputs": [],
      "source": [
        "class ClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        #inputs, classes = batch\n",
        "        images, targets = batch \n",
        "        images = images.type(torch.FloatTensor) # Uncomment for BreastCancer ClassfierBase class\n",
        "        #images = torch.reshape(images.type(torch.DoubleTensor), (len(images), 1))\n",
        "        targets = torch.reshape(targets.type(torch.FloatTensor), (len(targets), 1))\n",
        "        out = self(images)                      \n",
        "        loss = F.binary_cross_entropy(out, targets)      \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch\n",
        "        images = images.type(torch.FloatTensor) # Uncomment for BreastCancer ClassfierBase class\n",
        "        #images = torch.reshape(images.type(torch.DoubleTensor), (len(images), 1))\n",
        "        #print(images)\n",
        "        targets = torch.reshape(targets.type(torch.FloatTensor), (len(targets), 1))\n",
        "        #print(targets)\n",
        "        out = self(images)                           # Generate predictions\n",
        "        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n",
        "        score = F_score(out, targets)\n",
        "        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_scores = [x['val_score'] for x in outputs]\n",
        "        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 3: Define the DL model #####\n",
        "DNN1 was presented on 26.05, which shows an improvement in accuracy over epochs (all near .87). \n",
        "DNN2 is my updated version with a CNN architecture. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiABvc5rml2E"
      },
      "outputs": [],
      "source": [
        "# class DNN1(ClassificationBase):\n",
        "#     def __init__(self, input_size):\n",
        "#         super().__init__()\n",
        "#         self.linear = nn.Sequential(\n",
        "#             nn.Linear(input_size, 2048),\n",
        "#             nn.Dropout(p=0.2),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(2048, 1024),\n",
        "#             nn.Dropout(p=0.15),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(1024, 512),\n",
        "#             nn.Dropout(p=0.1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(512, 1)\n",
        "#         )\n",
        "#     #HERE THE MODEL PERFORMS A FORWARD PASS --> OUTPUT/PREDICTION\n",
        "#     def forward(self, xb):\n",
        "#         xb = xb.reshape(-1,input_size)\n",
        "#         xb = xb.to(torch.float32)  # Convert input to float32 data type\n",
        "#         out = self.linear(xb)\n",
        "#         #out = out.to(torch.float32) # Leave as comment for DNN 1\n",
        "#         return torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j27Gfq3-LWd"
      },
      "outputs": [],
      "source": [
        "class DNN4(ClassificationBase):\n",
        "    def __init__(self, input_shape, flat_shape):\n",
        "        super().__init__()\n",
        "        channels, height, width = input_shape\n",
        "        ## First convolutional layer\n",
        "        # \"Uses three filters to scan each row of the CyTOF data. This layer extracts relevant information from the cell marker profile of each cell.\" Is this grid AxBxC? Fix in inputShape[X]. Filter size = 1 x B\n",
        "        # We want to measure C markers. \n",
        "        # How many output markers\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=1, kernel_size=(25,25)) #(1,A)? - THE NUMBER OF NODES IN THE INPUT VECTOR. OR JUST KERNEL SIZE = 3?\n",
        "        self.bn1 = torch.nn.BatchNorm2d(1)\n",
        "        self.act1 = nn.ReLU()\n",
        "        \n",
        "\n",
        "        ## Second convolution layer\n",
        "        # The second convolution layer uses three filters to scan each row of the first layer's output. Each filter combines information from the first layer for each cell.\n",
        "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2,2)) \n",
        "        self.bn2 = torch.nn.BatchNorm2d(1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        ## Pooling layer\n",
        "        # \"The pooling layers averages the outputs of the second convolution layer. The purpose is to aggregate the cell level information into sample-level information.\"\"\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=2) \n",
        "        #self.pool1 = nn.AvgPool2d(kernel_size=3, stride=2) #1,1 would not change anything right? \n",
        "        self.flat = nn.Flatten() \n",
        "\n",
        "        ## Dense layer\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc1 = nn.Linear(in_features=7569, out_features=2048) #In features = , out features \n",
        "        #self.fc = nn.Linear(in_features=flat_shape, out_features=1)\n",
        "        # Better to make biggest jump here or reduce slowly?\n",
        "        self.bn3 = torch.nn.BatchNorm1d(2048)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.do1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc2 = nn.Linear(in_features=2048, out_features=1028) #In features = , out features \n",
        "        self.bn4 = torch.nn.BatchNorm1d(1028)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.do2 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc3 = nn.Linear(in_features=1028, out_features=512) #In features = , out features \n",
        "        self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "        self.act5 = nn.ReLU()\n",
        "        self.do3 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc4 = nn.Linear(in_features=512, out_features=256) #In features = , out features \n",
        "        self.bn6 = torch.nn.BatchNorm1d(256)\n",
        "        self.act6 = nn.ReLU()\n",
        "        self.do4 = nn.Dropout(p=0.1)\n",
        "\n",
        "        ## Output layer\n",
        "        # \"The output layer uses logistic regression to report the probability of CMV infection for each sample.\"\n",
        "        self.fc5 = nn.Linear(in_features=256, out_features=1)\n",
        "        #self.bn3 = nn.BatchNorm1d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input size\n",
        "        #print(\"Input dimensions\", x.shape)\n",
        "        #x = x.to(torch.float32)\n",
        "        x = x.float()\n",
        "        x = x.unsqueeze(1)\n",
        "        #print(\"Input dimensions\", x.shape)\n",
        "        out = self.conv1(x)\n",
        "        #print()\n",
        "        #print(\"Input dimensions conv1\", out.shape)\n",
        "        out = self.bn1(out)\n",
        "        #print(\"Input dimensions bn1\", out.shape)\n",
        "        out = self.act1(out)     \n",
        "        #print(\"Input dimensions act1\", out.shape)\n",
        "        out = self.conv2(out)\n",
        "        #print(\"Input dimensions conv2\", out.shape)\n",
        "        out = self.bn2(out)\n",
        "        #print(\"Input dimensions bn2\", out.shape)\n",
        "        out = self.act2(out)   \n",
        "        #print(\"Input dimensions act2\", out.shape)\n",
        "\n",
        "        out = self.pool1(out)\n",
        "        #print(\"Input dimensions pool1\", out.shape)\n",
        "        out = self.flat(out)\n",
        "        #print(\"Input dimensions flat\", out.shape)\n",
        "\n",
        "        #out = out.reshape(-1, input_size)\n",
        "        #print(\"Out dimensions\", out.shape)\n",
        "        out = self.fc1(out)  \n",
        "        #out = self.fc(out)  \n",
        "        #print(\"Input dimensions flat\", out.shape)    \n",
        "        out = self.act3(out)  \n",
        "        out = self.bn3(out) \n",
        "        out = self.do1(out)\n",
        "        #print(\"Input dimensions do1\", out.shape)   \n",
        "\n",
        "        out = self.fc2(out)  \n",
        "        #print(\"Input dimensions fc2\", out.shape)        \n",
        "        out = self.act4(out)   \n",
        "        out = self.bn4(out)\n",
        "        out = self.do2(out)\n",
        "\n",
        "        out = self.fc3(out)    \n",
        "        #print(\"Input dimensions fc3\", out.shape)    \n",
        "        out = self.act5(out)   \n",
        "        #out = self.sigmoid(out)  \n",
        "        out = self.bn5(out) \n",
        "        #print(\"Input dimensions bn3\", out.shape)     \n",
        "\n",
        "        out = self.fc4(out)      \n",
        "        out = self.act6(out)  \n",
        "        #print(\"Input dimensions fn4\", out.shape)\n",
        "        out = self.bn6(out)     \n",
        "        out = self.do4(out)\n",
        "        \n",
        "\n",
        "        out = self.fc5(out)     \n",
        "        #print(\"Input dimensions fn5\", out.shape)   \n",
        "        out = self.sigmoid(out)\n",
        "           \n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiyFxQE0Baa7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class DNN2(ClassificationBase):\n",
        "#     def __init__(self, input_shape, flat_shape):\n",
        "#         super().__init__()\n",
        "#         channels, height, width = input_shape\n",
        "#         ## First convolutional layer\n",
        "#         # \"Uses three filters to scan each row of the CyTOF data. This layer extracts relevant information from the cell marker profile of each cell.\" Is this grid AxBxC? Fix in inputShape[X]. Filter size = 1 x B\n",
        "#         # We want to measure C markers. \n",
        "#         # How many output markers\n",
        "#         self.conv1 = nn.Conv2d(in_channels=channels, out_channels=1, kernel_size=(2,2)) #(1,A)? - THE NUMBER OF NODES IN THE INPUT VECTOR. OR JUST KERNEL SIZE = 3?\n",
        "#         self.bn1 = torch.nn.BatchNorm2d(1)\n",
        "#         self.act1 = nn.ReLU()\n",
        "        \n",
        "#         ## Second convolution layer\n",
        "#         # The second convolution layer uses three filters to scan each row of the first layer's output. Each filter combines information from the first layer for each cell.\n",
        "#         self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2,2)) \n",
        "#         self.bn2 = torch.nn.BatchNorm2d(1)\n",
        "#         self.act2 = nn.ReLU()\n",
        "\n",
        "#         ## Pooling layer\n",
        "#         # \"The pooling layers averages the outputs of the second convolution layer. The purpose is to aggregate the cell level information into sample-level information.\"\"\n",
        "\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=2) \n",
        "#         #self.pool1 = nn.AvgPool2d(kernel_size=3, stride=2) #1,1 would not change anything right? \n",
        "#         self.flat = nn.Flatten() \n",
        "\n",
        "#         ## Dense layer\n",
        "#         # \"The dense layer further extracts information from the pooling layer.\"\n",
        "#         self.fc1 = nn.Linear(in_features=flat_shape, out_features=2048) #In features = , out features \n",
        "#         #self.fc = nn.Linear(in_features=flat_shape, out_features=1)\n",
        "#         # Better to make biggest jump here or reduce slowly?\n",
        "#         self.bn3 = torch.nn.BatchNorm1d(2048)\n",
        "#         self.act3 = nn.ReLU()\n",
        "#         self.do1 = nn.Dropout(p=0.1)\n",
        "\n",
        "#         # \"The dense layer further extracts information from the pooling layer.\"\n",
        "#         self.fc2 = nn.Linear(in_features=2048, out_features=1028) #In features = , out features \n",
        "#         self.bn4 = torch.nn.BatchNorm1d(1028)\n",
        "#         self.act4 = nn.ReLU()\n",
        "#         self.do2 = nn.Dropout(p=0.1)\n",
        "\n",
        "#         # \"The dense layer further extracts information from the pooling layer.\"\n",
        "#         self.fc3 = nn.Linear(in_features=1028, out_features=512) #In features = , out features \n",
        "#         self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "#         self.act5 = nn.ReLU()\n",
        "#         self.do3 = nn.Dropout(p=0.1)\n",
        "\n",
        "#         # \"The dense layer further extracts information from the pooling layer.\"\n",
        "#         self.fc4 = nn.Linear(in_features=512, out_features=256) #In features = , out features \n",
        "#         self.bn6 = torch.nn.BatchNorm1d(256)\n",
        "#         self.act6 = nn.ReLU()\n",
        "#         self.do4 = nn.Dropout(p=0.1)\n",
        "\n",
        "#         ## Output layer\n",
        "#         # \"The output layer uses logistic regression to report the probability of CMV infection for each sample.\"\n",
        "#         self.fc5 = nn.Linear(in_features=256, out_features=1)\n",
        "#         #self.bn3 = nn.BatchNorm1d(1)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Reshape input size\n",
        "#         #print(\"Input dimensions\", x.shape)\n",
        "#         #x = x.to(torch.float32)\n",
        "#         x = x.float()\n",
        "#         x = x.unsqueeze(1)\n",
        "#         #print(\"Input dimensions\", x.shape)\n",
        "#         out = self.conv1(x)\n",
        "#         ##print(\"Input dimensions conv1\", out.shape)\n",
        "#         out = self.bn1(out)\n",
        "#         ##print(\"Input dimensions bn1\", out.shape)\n",
        "#         out = self.act1(out)     \n",
        "#         #print(\"Input dimensions act1\", out.shape)\n",
        "\n",
        "#         out = self.conv2(out)\n",
        "#         #print(\"Input dimensions conv2\", out.shape)\n",
        "#         out = self.bn2(out)\n",
        "#         #print(\"Input dimensions bn2\", out.shape)\n",
        "#         out = self.act2(out)   \n",
        "#         #print(\"Input dimensions act2\", out.shape)\n",
        "\n",
        "#         out = self.pool1(out)\n",
        "#         #print(\"Input dimensions pool1\", out.shape)\n",
        "#         out = self.flat(out)\n",
        "#         #print(\"Input dimensions flat\", out.shape)\n",
        "\n",
        "#         #out = out.reshape(-1, input_size)\n",
        "#         #print(\"Out dimensions\", out.shape)\n",
        "#         out = self.fc1(out)  \n",
        "#         #out = self.fc(out)  \n",
        "#         #print(\"Input dimensions flat\", out.shape)    \n",
        "#         #flat_shape = out.shape[1]\n",
        "#         out = self.act3(out)  \n",
        "#         out = self.bn3(out) \n",
        "#         out = self.do1(out)\n",
        "#         #print(\"Input dimensions do1\", out.shape)   \n",
        "\n",
        "#         out = self.fc2(out)  \n",
        "#         #print(\"Input dimensions fc2\", out.shape)        \n",
        "#         out = self.act4(out)   \n",
        "#         out = self.bn4(out)\n",
        "#         out = self.do2(out)\n",
        "\n",
        "#         out = self.fc3(out)    \n",
        "#         #print(\"Input dimensions fc3\", out.shape)    \n",
        "#         out = self.act5(out)   \n",
        "#         #out = self.sigmoid(out)  \n",
        "#         out = self.bn5(out) \n",
        "#         #print(\"Input dimensions bn3\", out.shape)     \n",
        "\n",
        "#         out = self.fc4(out)      \n",
        "#         out = self.act6(out)  \n",
        "#         #print(\"Input dimensions fn4\", out.shape)\n",
        "#         out = self.bn6(out)     \n",
        "#         out = self.do4(out)\n",
        "        \n",
        "\n",
        "#         out = self.fc5(out)     \n",
        "#         #print(\"Input dimensions fn5\", out.shape)   \n",
        "#         out = self.sigmoid(out)\n",
        "           \n",
        "\n",
        "#         return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ75r4mL3dHf"
      },
      "outputs": [],
      "source": [
        "# class DNN3(ClassificationBase):\n",
        "#     def __init__(self, input_shape, flat_shape):\n",
        "#         super().__init__()\n",
        "#         channels, height, width = input_shape\n",
        "#         ## First convolutional layer\n",
        "#         # \"Uses three filters to scan each row of the CyTOF data. This layer extracts relevant information from the cell marker profile of each cell.\" Is this grid AxBxC? Fix in inputShape[X]. Filter size = 1 x B\n",
        "#         # We want to measure C markers. \n",
        "#         # How many output markers\n",
        "#         self.feature_extractor = nn.Sequential(\n",
        "#             nn.Conv2d(in_channels=channels, out_channels=1, kernel_size=(2,2)), #(1,A)? - THE NUMBER OF NODES IN THE INPUT VECTOR. OR JUST KERNEL SIZE = 3?, \n",
        "#             torch.nn.BatchNorm2d(1),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2,2)),\n",
        "#             torch.nn.BatchNorm2d(1),\n",
        "#             nn.ReLU()\n",
        "#         )\n",
        "\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
        "#             nn.Flatten(),\n",
        "#             nn.Linear(in_features=flat_shape, out_features=2048),\n",
        "#             torch.nn.BatchNorm1d(2048),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p=0.1),\n",
        "#             nn.Linear(in_features=2048, out_features=1024),\n",
        "#             torch.nn.BatchNorm1d(1024),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p=0.1),\n",
        "#             nn.Linear(in_features=1024, out_features=512),\n",
        "#             torch.nn.BatchNorm1d(512),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(p=0.1),\n",
        "#             nn.Linear(in_features=512, out_features=256),\n",
        "#             torch.nn.BatchNorm1d(256),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Linear(in_features=256, out_features=1)   \n",
        "#         )\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x.float()\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x = self.feature_extractor(x)\n",
        "#         x = self.classifier(x)\n",
        "#         out = self.sigmoid(x) ## If it is so close to .5... is that problematic?\n",
        "#         return out\n",
        "#         #return logits, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyGMfbxdOL1s"
      },
      "outputs": [],
      "source": [
        "# #Defining the convolutional neural network\n",
        "# # class LeNet5(ClassificationBase):\n",
        "# #     def __init__(self, input_shape, flat_shape):\n",
        "# #         super().__init__()\n",
        "# #         self.layer1 = nn.Sequential(\n",
        "# #             nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
        "# #             nn.BatchNorm2d(6),\n",
        "# #             nn.ReLU(),\n",
        "# #             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "# #         self.layer2 = nn.Sequential(\n",
        "# #             nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "# #             nn.BatchNorm2d(16),\n",
        "# #             nn.ReLU(),\n",
        "# #             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "# #         self.fc0 = nn.Linear(35344, 400)\n",
        "# #         self.relu0 = nn.ReLU()\n",
        "# #         self.fc = nn.Linear(400, 120)\n",
        "# #         self.relu = nn.ReLU()\n",
        "# #         self.fc1 = nn.Linear(120, 84)\n",
        "# #         self.relu1 = nn.ReLU()\n",
        "# #         self.fc2 = nn.Linear(84, 1)\n",
        "# #         #self.bn = nn.BatchNorm1d(1)\n",
        "# #         self.sigmoid = nn.Sigmoid()\n",
        "# #     def forward(self, x):\n",
        "# #         x = x.float()\n",
        "# #         x = x.unsqueeze(1)\n",
        "# #         out = self.layer1(x)\n",
        "# #         out = self.layer2(out)\n",
        "# #         out = out.reshape(out.size(0), -1)\n",
        "# #         out = self.fc0(out)\n",
        "# #         out = self.relu0(out)\n",
        "# #         out = self.fc(out)\n",
        "# #         out = self.relu(out)\n",
        "# #         out = self.fc1(out)\n",
        "# #         out = self.relu1(out)\n",
        "# #         out = self.fc2(out)\n",
        "# #         #out = self.bn(out)\n",
        "# #         out = self.sigmoid(out)\n",
        "# #         return out\n",
        "\n",
        "# #Defining the convolutional neural network\n",
        "# class LeNet5EfficientShallow(ClassificationBase):\n",
        "#     def __init__(self, input_shape, flat_shape):\n",
        "#         super().__init__()\n",
        "#         self.layer1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 3, kernel_size=5, stride=1, padding=0),\n",
        "#             nn.BatchNorm2d(3),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "#         self.layer2 = nn.Sequential(\n",
        "#             nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=0),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.ReLU(),\n",
        "#             nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "#         self.fc0 = nn.Linear(35344, 400)\n",
        "#         self.relu0 = nn.ReLU()\n",
        "#         self.fc = nn.Linear(400, 120)\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc1 = nn.Linear(120, 84)\n",
        "#         self.relu1 = nn.ReLU()\n",
        "#         self.fc2 = nn.Linear(84, 1)\n",
        "#         #self.bn = nn.BatchNorm1d(1)\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "#     def forward(self, x):\n",
        "#         x = x.float()\n",
        "#         x = x.unsqueeze(1)\n",
        "#         out = self.layer1(x)\n",
        "#         #print(out.shape)\n",
        "#         out = self.layer2(out)\n",
        "#         #print(out.shape)\n",
        "#         out = out.reshape(out.size(0), -1)\n",
        "#         out = self.fc0(out)\n",
        "#         out = self.relu0(out)\n",
        "#         out = self.fc(out)\n",
        "#         out = self.relu(out)\n",
        "#         out = self.fc1(out)\n",
        "#         out = self.relu1(out)\n",
        "#         out = self.fc2(out)\n",
        "#         #out = self.bn(out)\n",
        "#         out = self.sigmoid(out)\n",
        "#         return out"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 4. Set device and load data #####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhxDiuFBcKQ",
        "outputId": "a53ab05d-895e-4731-a396-b356afc7e3a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda') #REQUIRES CHANGING THE TORCH.FLOATTENSOR TO TORCH.CUDA.FLOATTENSOR \n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "    \n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3L_1C8FBeXH"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "val_dl = DeviceDataLoader(val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl65YYr_BgUP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up custom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "    \n",
        "    #writer = SummaryWriter()  # Create a SummaryWriter instance\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []  # learning rate\n",
        "        step = 0  # Initialize the step counter\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Write the training loss to TensorBoard with unique step for each batch\n",
        "            writer.add_scalar('Training Batch Loss', loss, step)\n",
        "            step += 1  # Increment the step counter\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Write the training loss and learning rate to TensorBoard\n",
        "        writer.add_scalar('Training Loss', torch.stack(train_losses).mean().item(), epoch)\n",
        "        writer.add_scalar('Learning Rate', lrs[-1], epoch)\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_scores(history):\n",
        "    scores = [x['val_score'] for x in history]\n",
        "    plt.plot(scores, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('score')\n",
        "    plt.title('F1 score vs. No. of epochs')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_scores_no_augmentation\")\n",
        "\n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_losses_no_augmentation\")\n",
        "\n",
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_lrs_no_augmentation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TypPxW1CN-1"
      },
      "outputs": [],
      "source": [
        "input=[1, 200, 200]\n",
        "input_size=200*200\n",
        "model = to_device(DNN4(input_shape=input, flat_shape=9801), device) #DONT WANT THIS TO BE AN INPUT!\n",
        "epochs = 100\n",
        "max_lr = 0.01\n",
        "opt_func = torch.optim.Adam\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Step 5. Training #####\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RF713TDCTMt",
        "outputId": "f01e76c9-d763-4c19-ab3d-e04ab7a16338"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'val_loss': 0.6820436120033264, 'val_score': 0.835106372833252}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run classifier \n",
        "history = [evaluate(model, val_dl)]\n",
        "history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvmRfBcdDABv",
        "outputId": "3cd5442a-cee6-43f7-a5d7-a45d87197eaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [01:40<00:00,  4.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [0], last_lr: 0.0004, train_loss: 0.7454, val_loss: 0.6432, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1], last_lr: 0.0005, train_loss: 0.6382, val_loss: 0.4475, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:25<00:00,  1.24s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2], last_lr: 0.0006, train_loss: 0.6097, val_loss: 0.4889, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:28<00:00,  1.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3], last_lr: 0.0008, train_loss: 0.5139, val_loss: 0.5011, val_score: 0.7589\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4], last_lr: 0.0010, train_loss: 0.4470, val_loss: 0.3298, val_score: 0.7713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5], last_lr: 0.0013, train_loss: 0.3981, val_loss: 0.3956, val_score: 0.7642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6], last_lr: 0.0016, train_loss: 0.3165, val_loss: 0.3768, val_score: 0.7713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7], last_lr: 0.0020, train_loss: 0.3021, val_loss: 0.4794, val_score: 0.6738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8], last_lr: 0.0024, train_loss: 0.3501, val_loss: 0.2815, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:25<00:00,  1.23s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9], last_lr: 0.0028, train_loss: 0.2604, val_loss: 0.2803, val_score: 0.7926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10], last_lr: 0.0032, train_loss: 0.1966, val_loss: 14.3405, val_score: 0.0053\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11], last_lr: 0.0037, train_loss: 0.2962, val_loss: 1.6265, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12], last_lr: 0.0042, train_loss: 0.2923, val_loss: 0.6477, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13], last_lr: 0.0047, train_loss: 0.3542, val_loss: 0.8246, val_score: 0.5621\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14], last_lr: 0.0052, train_loss: 0.2288, val_loss: 0.3037, val_score: 0.8067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15], last_lr: 0.0057, train_loss: 0.2383, val_loss: 0.8831, val_score: 0.4610\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16], last_lr: 0.0062, train_loss: 0.2461, val_loss: 0.2450, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17], last_lr: 0.0067, train_loss: 0.2237, val_loss: 0.2351, val_score: 0.7624\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:28<00:00,  1.35s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18], last_lr: 0.0071, train_loss: 0.1685, val_loss: 0.9948, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19], last_lr: 0.0076, train_loss: 0.2803, val_loss: 0.5887, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20], last_lr: 0.0080, train_loss: 0.1899, val_loss: 0.1925, val_score: 0.8245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [21], last_lr: 0.0084, train_loss: 0.1884, val_loss: 0.3516, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [22], last_lr: 0.0088, train_loss: 0.2217, val_loss: 0.1770, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [23], last_lr: 0.0091, train_loss: 0.1283, val_loss: 0.3480, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [24], last_lr: 0.0094, train_loss: 0.2132, val_loss: 2.0666, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [25], last_lr: 0.0096, train_loss: 0.3323, val_loss: 0.1911, val_score: 0.8262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [26], last_lr: 0.0098, train_loss: 0.2689, val_loss: 0.1711, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [27], last_lr: 0.0099, train_loss: 0.2714, val_loss: 0.3968, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [28], last_lr: 0.0100, train_loss: 0.3446, val_loss: 0.6837, val_score: 0.5869\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [29], last_lr: 0.0100, train_loss: 0.2416, val_loss: 0.1777, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:26<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [30], last_lr: 0.0100, train_loss: 0.1399, val_loss: 0.2822, val_score: 0.7642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:27<00:00,  1.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [31], last_lr: 0.0100, train_loss: 0.1620, val_loss: 0.2900, val_score: 0.8316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [32], last_lr: 0.0100, train_loss: 0.1816, val_loss: 0.6469, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [33], last_lr: 0.0099, train_loss: 0.1804, val_loss: 0.3334, val_score: 0.8316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [34], last_lr: 0.0099, train_loss: 0.1397, val_loss: 0.1475, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [35], last_lr: 0.0098, train_loss: 0.1080, val_loss: 0.4208, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [36], last_lr: 0.0098, train_loss: 0.0628, val_loss: 0.1202, val_score: 0.8121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [37], last_lr: 0.0097, train_loss: 0.0871, val_loss: 0.1180, val_score: 0.8138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [38], last_lr: 0.0096, train_loss: 0.0479, val_loss: 0.1970, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [39], last_lr: 0.0095, train_loss: 0.0665, val_loss: 0.0949, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [40], last_lr: 0.0094, train_loss: 0.0720, val_loss: 14.6638, val_score: 0.5337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [41], last_lr: 0.0093, train_loss: 0.2055, val_loss: 0.7300, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [42], last_lr: 0.0092, train_loss: 0.2029, val_loss: 0.1493, val_score: 0.8227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [43], last_lr: 0.0090, train_loss: 0.0904, val_loss: 0.1233, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [44], last_lr: 0.0089, train_loss: 0.0758, val_loss: 0.5663, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [45], last_lr: 0.0088, train_loss: 0.0788, val_loss: 0.5790, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [46], last_lr: 0.0086, train_loss: 0.0846, val_loss: 3.7026, val_score: 0.6418\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [47], last_lr: 0.0085, train_loss: 0.0916, val_loss: 2.3855, val_score: 0.7482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [48], last_lr: 0.0083, train_loss: 0.0438, val_loss: 0.2919, val_score: 0.6950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [49], last_lr: 0.0081, train_loss: 0.0988, val_loss: 0.4221, val_score: 0.6472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50], last_lr: 0.0079, train_loss: 0.0882, val_loss: 0.0922, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [51], last_lr: 0.0078, train_loss: 0.1009, val_loss: 0.1820, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [52], last_lr: 0.0076, train_loss: 0.2234, val_loss: 12.8810, val_score: 0.6188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [53], last_lr: 0.0074, train_loss: 0.2170, val_loss: 0.2177, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [54], last_lr: 0.0072, train_loss: 0.1662, val_loss: 63.5689, val_score: 0.0904\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [55], last_lr: 0.0070, train_loss: 0.0905, val_loss: 0.2539, val_score: 0.8121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [56], last_lr: 0.0068, train_loss: 0.0767, val_loss: 0.2401, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [57], last_lr: 0.0065, train_loss: 0.1563, val_loss: 0.3124, val_score: 0.6968\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [58], last_lr: 0.0063, train_loss: 0.0881, val_loss: 0.1125, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [59], last_lr: 0.0061, train_loss: 0.0390, val_loss: 0.0963, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [60], last_lr: 0.0059, train_loss: 0.0435, val_loss: 0.1429, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [61], last_lr: 0.0057, train_loss: 0.1021, val_loss: 35.9800, val_score: 0.4539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [62], last_lr: 0.0054, train_loss: 0.4943, val_loss: 7.6117, val_score: 0.7447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [63], last_lr: 0.0052, train_loss: 0.2935, val_loss: 0.3306, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [64], last_lr: 0.0050, train_loss: 0.1709, val_loss: 0.4182, val_score: 0.6365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [65], last_lr: 0.0048, train_loss: 0.1568, val_loss: 0.1874, val_score: 0.8351\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [66], last_lr: 0.0046, train_loss: 0.1703, val_loss: 0.2138, val_score: 0.8174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [67], last_lr: 0.0043, train_loss: 0.1493, val_loss: 0.1511, val_score: 0.8316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [68], last_lr: 0.0041, train_loss: 0.0743, val_loss: 0.1542, val_score: 0.8333\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [69], last_lr: 0.0039, train_loss: 0.1216, val_loss: 0.6624, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [70], last_lr: 0.0037, train_loss: 0.0776, val_loss: 0.1298, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [71], last_lr: 0.0035, train_loss: 0.0503, val_loss: 0.1186, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [72], last_lr: 0.0032, train_loss: 0.1055, val_loss: 0.6265, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:32<00:00,  1.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [73], last_lr: 0.0030, train_loss: 0.0382, val_loss: 0.6541, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:29<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [74], last_lr: 0.0028, train_loss: 0.0299, val_loss: 0.6347, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [75], last_lr: 0.0026, train_loss: 0.0247, val_loss: 0.6354, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [76], last_lr: 0.0024, train_loss: 0.0270, val_loss: 0.6415, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [77], last_lr: 0.0022, train_loss: 0.0501, val_loss: 0.6440, val_score: 0.8262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [78], last_lr: 0.0021, train_loss: 0.0179, val_loss: 0.6397, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [79], last_lr: 0.0019, train_loss: 0.0236, val_loss: 0.6460, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [80], last_lr: 0.0017, train_loss: 0.0362, val_loss: 0.6506, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [81], last_lr: 0.0015, train_loss: 0.0217, val_loss: 1.1562, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [82], last_lr: 0.0014, train_loss: 0.0405, val_loss: 1.1564, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [83], last_lr: 0.0012, train_loss: 0.0442, val_loss: 1.1694, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [84], last_lr: 0.0011, train_loss: 0.0210, val_loss: 0.6524, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [85], last_lr: 0.0010, train_loss: 0.0123, val_loss: 1.1876, val_score: 0.8262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.52s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [86], last_lr: 0.0008, train_loss: 0.0393, val_loss: 1.1704, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [87], last_lr: 0.0007, train_loss: 0.0238, val_loss: 0.6581, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [88], last_lr: 0.0006, train_loss: 0.0100, val_loss: 0.6429, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [89], last_lr: 0.0005, train_loss: 0.0250, val_loss: 0.6480, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.44s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [90], last_lr: 0.0004, train_loss: 0.0222, val_loss: 0.6433, val_score: 0.8245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [91], last_lr: 0.0003, train_loss: 0.0065, val_loss: 0.6438, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [92], last_lr: 0.0002, train_loss: 0.0164, val_loss: 0.6432, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [93], last_lr: 0.0002, train_loss: 0.0133, val_loss: 1.1554, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [94], last_lr: 0.0001, train_loss: 0.0190, val_loss: 0.6325, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.45s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [95], last_lr: 0.0001, train_loss: 0.0084, val_loss: 0.6510, val_score: 0.8280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.48s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [96], last_lr: 0.0000, train_loss: 0.0045, val_loss: 0.6411, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:30<00:00,  1.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [97], last_lr: 0.0000, train_loss: 0.0095, val_loss: 0.6200, val_score: 0.8262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:31<00:00,  1.49s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [98], last_lr: 0.0000, train_loss: 0.0146, val_loss: 1.1602, val_score: 0.8298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [00:32<00:00,  1.54s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [99], last_lr: 0.0000, train_loss: 0.0075, val_loss: 1.1580, val_score: 0.8280\n",
            "Total training time = 3733.0631201267242\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, opt_func=opt_func)\n",
        "train_time = time.time() - start_time\n",
        "total_train_time = time.time() - start_time\n",
        "print(\"Total training time =\", total_train_time)\n",
        "writer.flush\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvRr7IpsEl25",
        "outputId": "4b669c10-d796-44e6-de98-7f10587744af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-06-06 21:42:20.767046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "/content/runs/\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) \n",
            "To sign in with the TensorBoard uploader:\n",
            "\n",
            "1. On your computer or phone, visit:\n",
            "\n",
            "   https://www.google.com/device\n",
            "\n",
            "2. Sign in with your Google account, then enter:\n",
            "\n",
            "   RQS-LTH-RLF\n",
            "\n",
            "\n",
            "Upload started and will continue reading any new data as it's added to the logdir.\n",
            "\n",
            "To stop uploading, press Ctrl-C.\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/afvD6klQSyemRSkzAaAVPw/\n",
            "\n",
            "\u001b[1m[2023-06-06T21:42:39]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2023-06-06T21:42:40]\u001b[0m Total uploaded: 2300 scalars, 0 tensors, 0 binary objects\n"
          ]
        }
      ],
      "source": [
        "!yes|tensorboard dev upload --logdir /content/runs/ --name \"DNN2() with 25x25 \" --description \"CNN with 30/70 stratified split, kernel conv1\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wK52cMX_Oeom"
      },
      "source": [
        "# Model 01. For running with cells vs marker matrices.\n",
        "\n",
        "TODO: Adjust tensor sizes (maybe zero-padding, reducing, or augmenting?)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "84fgVKnnneBH"
      },
      "source": [
        "To run with tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYDGEWgznPXP",
        "outputId": "f760abca-8521-4db4-e829-2efd8b507b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qypBzOLPoU_c"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijP2u4gxA7fD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO3DlSTtA99K"
      },
      "outputs": [],
      "source": [
        "## From Hu et al. (2020)\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed; seed(111)\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_ind\n",
        "from IPython.display import Image\n",
        "\n",
        "## 0. Import\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "uC4dBKDCBAMU",
        "outputId": "f8408c52-c0b3-43a1-905f-f07870cf898f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d2d23db88abd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Initialize an empty list to store class labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ],
      "source": [
        "class AdDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.x = os.listdir(data_dir)\n",
        "        self.y = []  # Initialize an empty list to store class labels\n",
        "        self.dimensions = []  # Initialize an empty list to store dimensions\n",
        "\n",
        "        for file_name in self.x:\n",
        "          # Extract the part of the file name containing the label\n",
        "          label_part = file_name.split(\" \")[2]\n",
        "          # Extract the letter from the label part\n",
        "          y_label = label_part.split(\"_\")[0]\n",
        "          #print(y_label)\n",
        "          # Check if the class label is \"C\" and assign 0, else assign 1\n",
        "          if y_label == \"C\":\n",
        "              self.y.append(0)\n",
        "              #print(\"C\")\n",
        "          else:\n",
        "              self.y.append(1)\n",
        "              #print(\"P\")\n",
        "\n",
        "          file_path = os.path.join(self.data_dir, file_name)\n",
        "          data = pd.read_csv(file_path, sep=\",\", header=0).values\n",
        "          # if y_label==\"C\":\n",
        "\n",
        "    def __len__(self):\n",
        "        # Size of the whole data set\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        file_path = os.path.join(self.data_dir, self.x[idx])\n",
        "        data = pd.read_csv(file_path, sep=\"\\t\", header=None).values\n",
        "\n",
        "        # Select 10,000 random rows from the data\n",
        "        random_indices = random.sample(range(len(data)), 10000)\n",
        "        data = data[random_indices]\n",
        "        label = self.y[idx] # Retrieve corresponding labels\n",
        "\n",
        "        data = torch.from_numpy(data)\n",
        "        # print(\"Input shape should be\", shape)\n",
        "\n",
        "        return data, label\n",
        "\n",
        "    # def get_dimensions(self):\n",
        "    #     return self.dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX5rIey6BCaf",
        "outputId": "18bbf278-0d59-4fb2-d16b-650fe2208789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(44267, 35)\n",
            "(74892, 35)\n",
            "(10563, 35)\n",
            "(13414, 35)\n",
            "(178650, 35)\n",
            "(138863, 35)\n",
            "(115418, 35)\n",
            "(167620, 35)\n",
            "(294848, 35)\n",
            "(145652, 35)\n",
            "(129237, 35)\n",
            "(177182, 35)\n",
            "(133125, 35)\n",
            "(72079, 35)\n",
            "(20434, 35)\n",
            "(106058, 35)\n",
            "(298104, 35)\n",
            "(180851, 35)\n",
            "(2655, 35)\n",
            "(309058, 35)\n",
            "(274629, 35)\n"
          ]
        }
      ],
      "source": [
        "data_dir_train = \"/content/drive/MyDrive/colabData/ST1_20181025/train\" # 290\n",
        "train_dataset = AdDataset(data_dir_train)\n",
        "\n",
        "data_dir_val = \"/content/drive/MyDrive/colabData/ST1_20181025/validate\"\n",
        "val_dataset = AdDataset(data_dir_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=4, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt1kTgghBVsv"
      },
      "outputs": [],
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "    return F2.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plKfD9dSBXRP"
      },
      "outputs": [],
      "source": [
        "class ClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        #inputs, classes = batch\n",
        "        images, targets = batch\n",
        "        images = images.type(torch.FloatTensor) # Uncomment for BreastCancer ClassfierBase class\n",
        "        #images = torch.reshape(images.type(torch.DoubleTensor), (len(images), 1))\n",
        "        targets = torch.reshape(targets.type(torch.FloatTensor), (len(targets), 1))\n",
        "        out = self(images)\n",
        "        loss = F.binary_cross_entropy(out, targets)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch\n",
        "        images = images.type(torch.FloatTensor) # Uncomment for BreastCancer ClassfierBase class\n",
        "        #images = torch.reshape(images.type(torch.DoubleTensor), (len(images), 1))\n",
        "        #print(images)\n",
        "        targets = torch.reshape(targets.type(torch.FloatTensor), (len(targets), 1))\n",
        "        #print(targets)\n",
        "        out = self(images)                           # Generate predictions\n",
        "        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n",
        "        score = F_score(out, targets)\n",
        "        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_scores = [x['val_score'] for x in outputs]\n",
        "        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhxDiuFBcKQ",
        "outputId": "9aac6302-cfd8-437a-9900-de25c4ee0b35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda') #REQUIRES CHANGING THE TORCH.FLOATTENSOR TO TORCH.CUDA.FLOATTENSOR\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3L_1C8FBeXH"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "val_dl = DeviceDataLoader(val_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl65YYr_BgUP",
        "outputId": "62b25695-cfbe-4761-befb-3a363cfbcf64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    # Set up custom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "\n",
        "    #writer = SummaryWriter()  # Create a SummaryWriter instance\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []  # learning rate\n",
        "        step = 0  # Initialize the step counter\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            # Write the training loss to TensorBoard with unique step for each batch\n",
        "            writer.add_scalar('Training Batch Loss', loss, step)\n",
        "            step += 1  # Increment the step counter\n",
        "\n",
        "            # Gradient clipping\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        # Write the training loss and learning rate to TensorBoard\n",
        "        writer.add_scalar('Training Loss', torch.stack(train_losses).mean().item(), epoch)\n",
        "        writer.add_scalar('Learning Rate', lrs[-1], epoch)\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_scores(history):\n",
        "    scores = [x['val_score'] for x in history]\n",
        "    plt.plot(scores, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('score')\n",
        "    plt.title('F1 score vs. No. of epochs')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_scores_no_augmentation\")\n",
        "\n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_losses_no_augmentation\")\n",
        "\n",
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_lrs_no_augmentation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iM2hbPvfHWt"
      },
      "outputs": [],
      "source": [
        "class DNN2(ClassificationBase):\n",
        "    def __init__(self, input_channels, flat_shape):\n",
        "        super().__init__()\n",
        "        #channels, height, width = input_shape\n",
        "        ## First convolutional layer\n",
        "        # \"Uses three filters to scan each row of the CyTOF data. This layer extracts relevant information from the cell marker profile of each cell.\" Is this grid AxBxC? Fix in inputShape[X]. Filter size = 1 x B\n",
        "        # We want to measure C markers.\n",
        "        # How many output markers\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=1, kernel_size=(2,2)) #(1,A)? - THE NUMBER OF NODES IN THE INPUT VECTOR. OR JUST KERNEL SIZE = 3?\n",
        "        self.bn1 = torch.nn.BatchNorm2d(1)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        ## Second convolution layer\n",
        "        # The second convolution layer uses three filters to scan each row of the first layer's output. Each filter combines information from the first layer for each cell.\n",
        "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2,2))\n",
        "        self.bn2 = torch.nn.BatchNorm2d(1)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        ## Pooling layer\n",
        "        # \"The pooling layers averages the outputs of the second convolution layer. The purpose is to aggregate the cell level information into sample-level information.\"\"\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
        "        #self.pool1 = nn.AvgPool2d(kernel_size=3, stride=2) #1,1 would not change anything right?\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        ## Dense layer\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc1 = nn.Linear(in_features=flat_shape, out_features=2048) #In features = , out features\n",
        "        #self.fc = nn.Linear(in_features=flat_shape, out_features=1)\n",
        "        # Better to make biggest jump here or reduce slowly?\n",
        "        self.bn3 = torch.nn.BatchNorm1d(2048)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.do1 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc2 = nn.Linear(in_features=2048, out_features=1028) #In features = , out features\n",
        "        self.bn4 = torch.nn.BatchNorm1d(1028)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.do2 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc3 = nn.Linear(in_features=1028, out_features=512) #In features = , out features\n",
        "        self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "        self.act5 = nn.ReLU()\n",
        "        self.do3 = nn.Dropout(p=0.1)\n",
        "\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc4 = nn.Linear(in_features=512, out_features=256) #In features = , out features\n",
        "        self.bn6 = torch.nn.BatchNorm1d(256)\n",
        "        self.act6 = nn.ReLU()\n",
        "        self.do4 = nn.Dropout(p=0.1)\n",
        "\n",
        "        ## Output layer\n",
        "        # \"The output layer uses logistic regression to report the probability of CMV infection for each sample.\"\n",
        "        self.fc5 = nn.Linear(in_features=256, out_features=1)\n",
        "        #self.bn3 = nn.BatchNorm1d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input size\n",
        "        #print(\"Input dimensions\", x.shape)\n",
        "        #x = x.to(torch.float32)\n",
        "        x = x.float()\n",
        "        x = x.unsqueeze(1)\n",
        "        #print(\"Input dimensions\", x.shape)\n",
        "        out = self.conv1(x)\n",
        "        ##print(\"Input dimensions conv1\", out.shape)\n",
        "        out = self.bn1(out)\n",
        "        ##print(\"Input dimensions bn1\", out.shape)\n",
        "        out = self.act1(out)\n",
        "        #print(\"Input dimensions act1\", out.shape)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        #print(\"Input dimensions conv2\", out.shape)\n",
        "        out = self.bn2(out)\n",
        "        #print(\"Input dimensions bn2\", out.shape)\n",
        "        out = self.act2(out)\n",
        "        #print(\"Input dimensions act2\", out.shape)\n",
        "\n",
        "        out = self.pool1(out)\n",
        "        #print(\"Input dimensions pool1\", out.shape)\n",
        "        out = self.flat(out)\n",
        "        #print(\"Input dimensions flat\", out.shape)\n",
        "\n",
        "        #out = out.reshape(-1, input_size)\n",
        "        #print(\"Out dimensions\", out.shape)\n",
        "        out = self.fc1(out)\n",
        "        #out = self.fc(out)\n",
        "        #print(\"Input dimensions flat\", out.shape)\n",
        "        #flat_shape = out.shape[1]\n",
        "        out = self.act3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.do1(out)\n",
        "        #print(\"Input dimensions do1\", out.shape)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        #print(\"Input dimensions fc2\", out.shape)\n",
        "        out = self.act4(out)\n",
        "        out = self.bn4(out)\n",
        "        out = self.do2(out)\n",
        "\n",
        "        out = self.fc3(out)\n",
        "        #print(\"Input dimensions fc3\", out.shape)\n",
        "        out = self.act5(out)\n",
        "        #out = self.sigmoid(out)\n",
        "        out = self.bn5(out)\n",
        "        #print(\"Input dimensions bn3\", out.shape)\n",
        "\n",
        "        out = self.fc4(out)\n",
        "        out = self.act6(out)\n",
        "        #print(\"Input dimensions fn4\", out.shape)\n",
        "        out = self.bn6(out)\n",
        "        out = self.do4(out)\n",
        "\n",
        "\n",
        "        out = self.fc5(out)\n",
        "        #print(\"Input dimensions fn5\", out.shape)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TypPxW1CN-1",
        "outputId": "5faeea9c-62e5-47a9-9dd1-2988e6be54fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<itertools.product at 0x7fb268701d80>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#input=[1, 200, 200]\n",
        "# input_size=200*200\n",
        "# model = to_device(DNN1(input_shape=input), device) #, flat_shape=9801), device) #DONT WANT THIS TO BE AN INPUT!\n",
        "# epochs = 100\n",
        "# max_lr = 0.01\n",
        "opt_func = torch.optim.Adam\n",
        "\n",
        "# Run classifier\n",
        "# history = [evaluate(model, val_dl)]\n",
        "# history\n",
        "\n",
        "# Train\n",
        "# start_time = time.time()\n",
        "# history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, opt_func=opt_func)\n",
        "# train_time = time.time() - start_time\n",
        "# # total_train_time = time.time() - start_time\n",
        "# print(\"Total training time =\", total_train_time)\n",
        "# writer.flush\n",
        "# writer.close()\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "# Define the hyperparameter values to explore\n",
        "epochs_values = [1, 2, 3]\n",
        "max_lr_values = [0.01, 0.001]\n",
        "\n",
        "best_score = 0.0\n",
        "best_epochs = 0\n",
        "best_max_lr = 0.0\n",
        "\n",
        "product(epochs_values, max_lr_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw_KaRi3I2sm"
      },
      "outputs": [],
      "source": [
        "for epochs, max_lr in product(epochs_values, max_lr_values):\n",
        "\n",
        "    # Create a unique tag for each run based on the hyperparameters\n",
        "    tag = f\"epochs_{epochs}_max_lr_{max_lr}\"\n",
        "\n",
        "    # Create a SummaryWriter instance for each run\n",
        "    writer = SummaryWriter(log_dir=f\"runs/{tag}\")\n",
        "\n",
        "    # Create a new instance of the model for each combination of hyperparameters\n",
        "    model = to_device(DNN2(input_channels=1, flat_shape=9801), device)\n",
        "    #model = to_device(DNN(), device)\n",
        "\n",
        "    # Train the model and evaluate its performance\n",
        "    history = [evaluate(model, val_dl)]\n",
        "    history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, opt_func=opt_func)\n",
        "\n",
        "    # Calculate the validation score\n",
        "    final_score = history[-1]['val_score']\n",
        "\n",
        "    # Check if the current combination is the best\n",
        "    if final_score > best_score:\n",
        "        print(\"Better parameters found, updating.\")\n",
        "        best_score = final_score\n",
        "        best_epochs = epochs\n",
        "        best_max_lr = max_lr\n",
        "\n",
        "    # Print the validation score for the current combination\n",
        "    print(f\"Epochs: {epochs}, Max LR: {max_lr}, Validation Score: {final_score}\")\n",
        "\n",
        "    # Close the writer for each run\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Epochs:\", best_epochs)\n",
        "print(\"Max LR:\", best_max_lr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5IQBbNSAL1E9"
      },
      "source": [
        "If above works, add tensor board to see output.\n",
        "Then change architecture and num epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qvRr7IpsEl25"
      },
      "outputs": [],
      "source": [
        "!yes|tensorboard dev upload --logdir /content/runs/ --name \"DNN2() with 25x25 \" --description \"CNN with 30/70 stratified split, kernel conv1\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

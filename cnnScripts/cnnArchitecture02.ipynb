{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Notebook\n",
        "Inputs = dataloader objects from Data.py\n",
        "\\\n",
        "Outputs = model training tensorboard dashboards\n"
      ],
      "metadata": {
        "id": "0oYMgk-IRiEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Import"
      ],
      "metadata": {
        "id": "nzzsMcq2TCGm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84fgVKnnneBH"
      },
      "source": [
        "To run with tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYDGEWgznPXP",
        "outputId": "43f81b70-ecd5-4e03-afe1-ee3d0ea3df1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "qypBzOLPoU_c"
      },
      "outputs": [],
      "source": [
        "#!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "ijP2u4gxA7fD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "AO3DlSTtA99K"
      },
      "outputs": [],
      "source": [
        "## From Hu et al. (2020)\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import seed; seed(111)\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ttest_ind\n",
        "from IPython.display import Image\n",
        "\n",
        "## 0. Import\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load data"
      ],
      "metadata": {
        "id": "vhAMtvTmTGFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "uC4dBKDCBAMU"
      },
      "outputs": [],
      "source": [
        "class AdDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        ## Build a list of tuples\n",
        "        # Here x is our .csv files (i.e. CyTOF data)\n",
        "        # Here y is our output (0 for a control and 1 for a AD patient)\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.x = os.listdir(data_dir)\n",
        "        self.y= []  # Initialize an empty list to store class labels\n",
        "\n",
        "        for file_name in self.x:\n",
        "            # Extract the letter preceding \".csv\" in the file name\n",
        "            y_label = file_name.split(\".\")[0][-1]\n",
        "            # Check if the class label is \"C\" and assign 0, else assign 1\n",
        "            if y_label == \"C\":\n",
        "                self.y.append(0)\n",
        "                #print(0)\n",
        "            else:\n",
        "                self.y.append(1)\n",
        "                #print(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        ## Size of whole data set\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ## For loading data on demand, rather than loaded in __init__ step, to increase memory inefficiency.\n",
        "\n",
        "        file_path = os.path.join(self.data_dir, self.x[idx])\n",
        "        data = pd.read_csv(file_path, sep=\"\\t\", header=None).values\n",
        "        data = torch.from_numpy(data)\n",
        "        label = self.y[idx]  # Get the class label for the corresponding file WATCH OUT FOR FLOAT --> MAY CAUSE ERRORS BECAUSE DATA NOT IN SAME DTYPE AS CLASS_LABEL\n",
        "        #dimensions = data.shape  # Get the dimensions of the data\n",
        "\n",
        "        return data, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "HX5rIey6BCaf"
      },
      "outputs": [],
      "source": [
        "data_dir_train = \"/content/drive/MyDrive/colabData/st1/train\" # 290\n",
        "train_dataset = AdDataset(data_dir_train)\n",
        "\n",
        "\n",
        "data_dir_val = \"/content/drive/MyDrive/colabData/st1/validate\"\n",
        "val_dataset = AdDataset(data_dir_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=4, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model performance"
      ],
      "metadata": {
        "id": "_GFkqEGDS3DB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "plKfD9dSBXRP"
      },
      "outputs": [],
      "source": [
        "class ClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        #inputs, classes = batch\n",
        "        images, targets = batch\n",
        "        images = images.type(torch.FloatTensor) # Uncomment for BreastCancer ClassfierBase class\n",
        "        #images = torch.reshape(images.type(torch.DoubleTensor), (len(images), 1))\n",
        "        targets = torch.reshape(targets.type(torch.FloatTensor), (len(targets), 1))\n",
        "        out = self(images)\n",
        "        loss = F.binary_cross_entropy(out, targets)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, targets = batch\n",
        "        images = images.type(torch.FloatTensor) # Uncomment for BreastCancer ClassfierBase class\n",
        "        #images = torch.reshape(images.type(torch.DoubleTensor), (len(images), 1))\n",
        "        #print(images)\n",
        "        targets = torch.reshape(targets.type(torch.FloatTensor), (len(targets), 1))\n",
        "        #print(targets)\n",
        "        out = self(images)                           # Generate predictions\n",
        "        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n",
        "        score = F_score(out, targets)\n",
        "        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_scores = [x['val_score'] for x in outputs]\n",
        "        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_score']))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to evaluate"
      ],
      "metadata": {
        "id": "nyZFYwZZSl-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "jt1kTgghBVsv"
      },
      "outputs": [],
      "source": [
        "def F_score(output, label, threshold=0.5, beta=1):\n",
        "    prob = output > threshold\n",
        "    label = label > threshold\n",
        "\n",
        "    TP = (prob & label).sum(1).float()\n",
        "    TN = ((~prob) & (~label)).sum(1).float()\n",
        "    FP = (prob & (~label)).sum(1).float()\n",
        "    FN = ((~prob) & label).sum(1).float()\n",
        "\n",
        "    precision = torch.mean(TP / (TP + FP + 1e-12))\n",
        "    recall = torch.mean(TP / (TP + FN + 1e-12))\n",
        "    F2 = (1 + beta**2) * precision * recall / (beta**2 * precision + recall + 1e-12)\n",
        "    return F2.mean(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl65YYr_BgUP",
        "outputId": "a8eb8279-26cf-4871-d4b8-b500cab7a0a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    # Set up custom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "\n",
        "    #writer = SummaryWriter()  # Create a SummaryWriter instance\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []  # learning rate\n",
        "        step = 0  # Initialize the step counter\n",
        "        for batch in tqdm(train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            # Write the training loss to TensorBoard with unique step for each batch\n",
        "            writer.add_scalar('Training Batch Loss', loss, step)\n",
        "            step += 1  # Increment the step counter\n",
        "\n",
        "            # Gradient clipping\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        # Write the training loss and learning rate to TensorBoard\n",
        "        writer.add_scalar('Training Loss', torch.stack(train_losses).mean().item(), epoch)\n",
        "        writer.add_scalar('Learning Rate', lrs[-1], epoch)\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_scores(history):\n",
        "    scores = [x['val_score'] for x in history]\n",
        "    plt.plot(scores, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('score')\n",
        "    plt.title('F1 score vs. No. of epochs')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_scores_no_augmentation\")\n",
        "\n",
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, '-bx')\n",
        "    plt.plot(val_losses, '-rx')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. No. of epochs')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_losses_no_augmentation\")\n",
        "\n",
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.')\n",
        "    plt.show()\n",
        "    #plt.savefig(\"DNN_lrs_no_augmentation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load device\n"
      ],
      "metadata": {
        "id": "lo8evLZjSgFA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhxDiuFBcKQ",
        "outputId": "9a3c21e4-ab39-4430-86f2-54119c3451b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda') #REQUIRES CHANGING THE TORCH.FLOATTENSOR TO TORCH.CUDA.FLOATTENSOR\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ],
      "metadata": {
        "id": "9uKOdXuoSjNy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "U3L_1C8FBeXH"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "val_dl = DeviceDataLoader(val_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Baseline accuracy"
      ],
      "metadata": {
        "id": "J-ieAaBnSSsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 FC model"
      ],
      "metadata": {
        "id": "__G4T4EqT38L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Input_size in forward is hardcoded as grid size"
      ],
      "metadata": {
        "id": "NhNk8kM_iZ6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "LiABvc5rml2E"
      },
      "outputs": [],
      "source": [
        "class FCNN(ClassificationBase):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(input_size, 2048),\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.Dropout(p=0.15),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "    #HERE THE MODEL PERFORMS A FORWARD PASS --> OUTPUT/PREDICTION\n",
        "    def forward(self, xb, input_size=200*200):\n",
        "        xb = xb.reshape(-1,input_size)\n",
        "        xb = xb.to(torch.float32)  # Convert input to float32 data type\n",
        "        out = self.linear(xb)\n",
        "        #out = out.to(torch.float32) # Leave as comment for DNN 1\n",
        "        return torch.sigmoid(out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Run model\n",
        "Saves output in tensorboard"
      ],
      "metadata": {
        "id": "VnItqCm1T7nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Initialize"
      ],
      "metadata": {
        "id": "kLJCAsQuUMck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()\n",
        "model = to_device(FCNN(input_size=200*200), device) #, flat_shape=9801), device) #DONT WANT THIS TO BE AN INPUT!\n",
        "epochs = 100\n",
        "max_lr = 0.01\n",
        "opt_func = torch.optim.Adam"
      ],
      "metadata": {
        "id": "5bh2RuAmUIyH"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Run classifier"
      ],
      "metadata": {
        "id": "p1MfsgLKURZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = [evaluate(model, val_dl)]\n",
        "history"
      ],
      "metadata": {
        "id": "Z5KGmcJBUIle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b865208-0f9d-45f1-b68a-1c8be7f1a3b3"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'val_loss': 0.6887596845626831, 'val_score': 0.8333332538604736}]"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3 Train"
      ],
      "metadata": {
        "id": "BUm2_vyHUWYC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "2TypPxW1CN-1"
      },
      "outputs": [],
      "source": [
        "# start_time = time.time()\n",
        "# history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, opt_func=opt_func)\n",
        "# train_time = time.time() - start_time\n",
        "# total_train_time = time.time() - start_time\n",
        "# print(\"Total training time =\", total_train_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3 Evaluate"
      ],
      "metadata": {
        "id": "yAKNmKbvUfK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust name and description as needed"
      ],
      "metadata": {
        "id": "wOBbMgcLUucb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# writer.flush\n",
        "# writer.close()"
      ],
      "metadata": {
        "id": "JESQO4ZvUhyc"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "qvRr7IpsEl25"
      },
      "outputs": [],
      "source": [
        "# !yes|tensorboard dev upload --logdir /content/runs/ --name \"DNN() Fully Connected \" --description \"Fully connected to evaluate baseline accuracy\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. GridSearch with fully connected layers"
      ],
      "metadata": {
        "id": "xmaAv0DYmuRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "2SvtJ0udmuRa"
      },
      "outputs": [],
      "source": [
        "# from itertools import product\n",
        "\n",
        "# # Define the hyperparameter values to explore\n",
        "# epochs_values = [1, 2, 3]\n",
        "# max_lr_values = [0.01, 0.001]\n",
        "\n",
        "# best_score = 0.0\n",
        "# best_epochs = 0\n",
        "# best_max_lr = 0.0\n",
        "\n",
        "# product(epochs_values, max_lr_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "icxCsAidmuRa"
      },
      "outputs": [],
      "source": [
        "# for epochs, max_lr in product(epochs_values, max_lr_values):\n",
        "\n",
        "#     # Create a unique tag for each run based on the hyperparameters\n",
        "#     tag = f\"epochs_{epochs}_max_lr_{max_lr}\"\n",
        "\n",
        "#     # Create a SummaryWriter instance for each run\n",
        "#     writer = SummaryWriter(log_dir=f\"runs/{tag}\")\n",
        "\n",
        "#     # Create a new instance of the model for each combination of hyperparameters\n",
        "#     model = to_device(DNN(input_size=200*200), device) #, flat_shape=9801), device) #DONT WANT THIS TO BE AN INPUT!\n",
        "\n",
        "#     # Train the model and evaluate its performance\n",
        "#     history = [evaluate(model, val_dl)]\n",
        "#     history += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, opt_func=opt_func)\n",
        "\n",
        "#     # Calculate the validation score\n",
        "#     final_score = history[-1]['val_score']\n",
        "\n",
        "#     # Check if the current combination is the best\n",
        "#     if final_score > best_score:\n",
        "#         print(\"Better parameters found, updating.\")\n",
        "#         best_score = final_score\n",
        "#         best_epochs = epochs\n",
        "#         best_max_lr = max_lr\n",
        "\n",
        "#     # Print the validation score for the current combination\n",
        "#     print(f\"Epochs: {epochs}, Max LR: {max_lr}, Validation Score: {final_score}\")\n",
        "\n",
        "#     # Close the writer for each run\n",
        "#     writer.close()\n",
        "\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(\"Best Hyperparameters:\")\n",
        "# print(\"Epochs:\", best_epochs)\n",
        "# print(\"Max LR:\", best_max_lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!yes|tensorboard dev upload --logdir /content/runs/ --name \"DNN() GRIDSearch \" --description \"Fully connected to evaluate baseline accuracy\""
      ],
      "metadata": {
        "id": "Yk25X9nmsTlp"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. GridSearch with convolutional layers\n",
        "Right now a limitation is the changing outputs/mismatch between convolutional layer sizes. Have just changed it to make it consistent.\n",
        "I.e. Size of layer 1 == size of layer 2."
      ],
      "metadata": {
        "id": "bSe6_k0PSY--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNGridSearch(ClassificationBase):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        conv1_filters,\n",
        "        conv1_kernel_size,\n",
        "        conv2_filters,\n",
        "        conv2_kernel_size,\n",
        "        maxpool_kernel_size,\n",
        "        dropout,\n",
        "        fc1_nodes,\n",
        "        #use_second_dense,\n",
        "        fc2_nodes,\n",
        "        #use_third_dense,\n",
        "        fc3_nodes,\n",
        "        use_second_conv_block,\n",
        "        flat_features,\n",
        "        conv_stride,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        channels, height, width = input_shape\n",
        "        ## First convolutional layer\n",
        "        # \"Uses three filters to scan each row of the CyTOF data. This layer extracts relevant information from the cell marker profile of each cell.\" Is this grid AxBxC? Fix in inputShape[X]. Filter size = 1 x B\n",
        "        # We want to measure C markers.\n",
        "        # How many output markers\n",
        "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=conv1_filters, kernel_size=conv1_kernel_size) #(1,A)? - THE NUMBER OF NODES IN THE INPUT VECTOR. OR JUST KERNEL SIZE = 3?\n",
        "        self.bn1 = torch.nn.BatchNorm2d(conv1_filters)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        ## Second (optional) convolutional layer\n",
        "        self.use_second_conv_block = use_second_conv_block\n",
        "        if self.use_second_conv_block:\n",
        "            self.conv2 = nn.Conv2d(in_channels = conv1_filters, out_channels=conv2_filters, kernel_size=conv2_kernel_size)\n",
        "            self.bn2 = torch.nn.BatchNorm2d(conv2_filters)\n",
        "            self.act2 = nn.ReLU()\n",
        "\n",
        "            ## Pooling layer\n",
        "            # \"The pooling layers averages the outputs of the second convolution layer. The purpose is to aggregate the cell level information into sample-level information.\"\"\n",
        "            self.pool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=conv_stride)\n",
        "            self.flat = nn.Flatten()\n",
        "        else:\n",
        "\n",
        "          ## Pooling layer\n",
        "          # \"The pooling layers averages the outputs of the first convolution layer. The purpose is to aggregate the cell level information into sample-level information.\"\"\n",
        "          self.pool = nn.MaxPool2d(kernel_size=maxpool_kernel_size, stride=conv_stride)\n",
        "          self.flat = nn.Flatten()\n",
        "\n",
        "        ## Dense layers\n",
        "        # \"The dense layer further extracts information from the pooling layer.\"\n",
        "        self.fc1 = nn.Linear(in_features=flat_features, out_features=fc1_nodes) #flat_features = 10\n",
        "        self.bn3 = torch.nn.BatchNorm1d(fc1_nodes)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.do1 = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Second dense layer\n",
        "        self.fc2 = nn.Linear(in_features=fc1_nodes, out_features=fc2_nodes)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(fc2_nodes)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.do2 = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Third dense layer\n",
        "        self.fc3 = nn.Linear(in_features=fc2_nodes, out_features=fc3_nodes)\n",
        "        self.bn5 = torch.nn.BatchNorm1d(fc3_nodes)\n",
        "        self.act5 = nn.ReLU()\n",
        "        self.do3 = nn.Dropout(p=dropout)\n",
        "\n",
        "        ## Output layer\n",
        "        # \"Uses logistic regression to report the probability of AD for each sample.\"\n",
        "        self.fc5 = nn.Linear(in_features=fc3_nodes, out_features=1)\n",
        "        self.bn6 = nn.BatchNorm1d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        x = x.unsqueeze(1)\n",
        "        #print(\"Input dimensions\", x.shape)\n",
        "        x = self.act1(self.bn1(self.conv1(x)))\n",
        "        #print(\"Input dimensions 1st conv layer\", x.shape)\n",
        "        if self.use_second_conv_block:\n",
        "            x = self.act2(self.bn2(self.conv2(x)))\n",
        "            x = self.flat(self.pool(x))\n",
        "            #print(\"Input dimensions flat features conv2\", x.shape)\n",
        "        else:\n",
        "            x = self.flat(self.pool(x))\n",
        "            #print(\"Input dimensions flat features conv1\", x.shape)\n",
        "\n",
        "        x = self.do1(self.act3(self.bn3(self.fc1(x))))\n",
        "        x = self.do2(self.act4(self.bn4(self.fc2(x))))\n",
        "        x = self.do3(self.act5(self.bn5(self.fc3(x))))\n",
        "        out = self.bn6(self.fc5(x))\n",
        "        return self.sigmoid(out)\n"
      ],
      "metadata": {
        "id": "RurkbcIv3N61"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "Sbbdf3CZIvfe"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "input_shape = [1,200,200]\n",
        "# Define the hyperparameter values to explore\n",
        "epochs_values = [50, 100]\n",
        "max_lr_values = [0.01, 0.001]\n",
        "input_shape = [1,200,200]\n",
        "#conv1_filters = [1,3, 5]\n",
        "#conv1_kernel_size = [2, 5, 9]\n",
        "conv_filters = [1, 3, 5]\n",
        "conv_kernel_size = [2, 5, 9]\n",
        "#conv_stride = [2]\n",
        "#maxpool_kernel_size = [2]\n",
        "conv2_filters = [1,3, 5]\n",
        "conv2_kernel_size = [2, 5, 9]\n",
        "#dropout = [0.1]\n",
        "#fc1_nodes = [2048]\n",
        "#fc2_nodes = [512]\n",
        "#fc3_nodes = [128]\n",
        "use_second_conv_block = [True, False]\n",
        "\n",
        "best_score = 0.0\n",
        "best_epochs = 0\n",
        "best_max_lr = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to calculate the number of flat features required in the dense layers.\n",
        "Note that filter sizes must be square (W = H)"
      ],
      "metadata": {
        "id": "2ySDmxznkLLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_flat_features(input_shape, conv_kernel_size, conv_stride, use_second_conv_block):\n",
        "    channels, height, width = input_shape\n",
        "\n",
        "    # First convolutional layer\n",
        "    conv1_output = ((height - conv_kernel_size)/conv_stride)+1\n",
        "    print(conv1_output)\n",
        "    if use_second_conv_block:\n",
        "        # Flat features size after pooling\n",
        "        conv2_output = ((conv1_output - conv_kernel_size)/conv_stride)+1\n",
        "        flat_features = conv2_output**2\n",
        "    else:\n",
        "        # Flat features size after pooling\n",
        "        flat_features = conv1_output**2\n",
        "\n",
        "    return int(flat_features)"
      ],
      "metadata": {
        "id": "jnNGdS-ukKMm"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_flat_features(input_shape, 1, 1, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY5QWXChlK8v",
        "outputId": "1ea62906-176a-4079-e9e3-03d9fa7985ec"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run models"
      ],
      "metadata": {
        "id": "B4dsG-lnkQIx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw_KaRi3I2sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e0ff36-31ec-4573-afd1-8fa4be67421a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 50_0.01_1_2_True\n",
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], last_lr: 0.0005, train_loss: 0.7690, val_loss: 0.7184, val_score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1], last_lr: 0.0008, train_loss: 0.6185, val_loss: 0.8125, val_score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2], last_lr: 0.0013, train_loss: 0.5622, val_loss: 0.5755, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3], last_lr: 0.0020, train_loss: 0.5153, val_loss: 0.5086, val_score: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4], last_lr: 0.0028, train_loss: 0.4709, val_loss: 0.6127, val_score: 0.6152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5], last_lr: 0.0037, train_loss: 0.4804, val_loss: 0.5835, val_score: 0.7376\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6], last_lr: 0.0047, train_loss: 0.4389, val_loss: 0.4033, val_score: 0.8067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7], last_lr: 0.0057, train_loss: 0.4433, val_loss: 0.4779, val_score: 0.6986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8], last_lr: 0.0067, train_loss: 0.3837, val_loss: 0.4235, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9], last_lr: 0.0076, train_loss: 0.3529, val_loss: 0.5248, val_score: 0.6348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10], last_lr: 0.0084, train_loss: 0.2852, val_loss: 0.3240, val_score: 0.8245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11], last_lr: 0.0091, train_loss: 0.2739, val_loss: 0.3641, val_score: 0.8121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12], last_lr: 0.0096, train_loss: 0.3067, val_loss: 0.7077, val_score: 0.5833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13], last_lr: 0.0099, train_loss: 0.2904, val_loss: 1.1778, val_score: 0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14], last_lr: 0.0100, train_loss: 0.2393, val_loss: 0.5250, val_score: 0.8280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15], last_lr: 0.0100, train_loss: 0.1609, val_loss: 0.3130, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:11<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16], last_lr: 0.0099, train_loss: 0.1626, val_loss: 0.3815, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17], last_lr: 0.0098, train_loss: 0.1779, val_loss: 0.1804, val_score: 0.8121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18], last_lr: 0.0097, train_loss: 0.1983, val_loss: 0.5927, val_score: 0.6809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19], last_lr: 0.0095, train_loss: 0.1518, val_loss: 0.4228, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20], last_lr: 0.0093, train_loss: 0.1662, val_loss: 0.4722, val_score: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21], last_lr: 0.0090, train_loss: 0.1572, val_loss: 0.1355, val_score: 0.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22], last_lr: 0.0088, train_loss: 0.1348, val_loss: 0.2096, val_score: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23], last_lr: 0.0085, train_loss: 0.1381, val_loss: 0.2274, val_score: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24], last_lr: 0.0081, train_loss: 0.1143, val_loss: 0.1677, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25], last_lr: 0.0078, train_loss: 0.1191, val_loss: 0.2772, val_score: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26], last_lr: 0.0074, train_loss: 0.1547, val_loss: 0.1367, val_score: 0.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27], last_lr: 0.0070, train_loss: 0.0974, val_loss: 0.1190, val_score: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28], last_lr: 0.0065, train_loss: 0.1014, val_loss: 0.1445, val_score: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29], last_lr: 0.0061, train_loss: 0.1082, val_loss: 0.1002, val_score: 0.8316\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30], last_lr: 0.0057, train_loss: 0.1061, val_loss: 0.1123, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31], last_lr: 0.0052, train_loss: 0.0724, val_loss: 0.1192, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32], last_lr: 0.0048, train_loss: 0.1103, val_loss: 0.1412, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33], last_lr: 0.0043, train_loss: 0.0906, val_loss: 0.2095, val_score: 0.7713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34], last_lr: 0.0039, train_loss: 0.0917, val_loss: 0.0973, val_score: 0.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35], last_lr: 0.0035, train_loss: 0.0574, val_loss: 0.1193, val_score: 0.8298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36], last_lr: 0.0030, train_loss: 0.0594, val_loss: 0.0974, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37], last_lr: 0.0026, train_loss: 0.0810, val_loss: 0.1404, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38], last_lr: 0.0022, train_loss: 0.0580, val_loss: 0.3052, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39], last_lr: 0.0019, train_loss: 0.1059, val_loss: 0.1000, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40], last_lr: 0.0015, train_loss: 0.1105, val_loss: 0.0853, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41], last_lr: 0.0012, train_loss: 0.1011, val_loss: 0.1260, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42], last_lr: 0.0010, train_loss: 0.0982, val_loss: 0.5708, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43], last_lr: 0.0007, train_loss: 0.0573, val_loss: 0.5313, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44], last_lr: 0.0005, train_loss: 0.1324, val_loss: 0.2071, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45], last_lr: 0.0003, train_loss: 0.0993, val_loss: 0.1380, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46], last_lr: 0.0002, train_loss: 0.1213, val_loss: 0.0999, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47], last_lr: 0.0001, train_loss: 0.1021, val_loss: 0.1044, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48], last_lr: 0.0000, train_loss: 0.0953, val_loss: 0.0979, val_score: 0.8351\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:12<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49], last_lr: 0.0000, train_loss: 0.1007, val_loss: 0.0957, val_score: 0.8351\n",
            "Better parameters found, updating.\n",
            "Epochs: 50, Max LR: 0.01, Validation Score: 0.835106372833252\n",
            "Running 50_0.01_1_2_False\n",
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21/21 [00:13<00:00,  1.59it/s]\n"
          ]
        }
      ],
      "source": [
        "for epochs, max_lr, conv_filter, conv_kernel, use_second in product(epochs_values, max_lr_values, conv_filters, conv_kernel_size, use_second_conv_block):\n",
        "#for epochs, max_lr, conv1_filter, conv1_kernel, conv2_filter, conv2_kernel, use_second in product(epochs_values, max_lr_values, conv1_filters, conv1_kernel_size, conv2_filters, conv2_kernel_size, use_second_conv_block):\n",
        "\n",
        "    # Create a unique tag for each run based on the hyperparameters\n",
        "    tag = f\"{epochs}_{max_lr}_{conv_filter}_{conv_kernel}_{use_second}\"\n",
        "    print(\"Running\", tag)\n",
        "\n",
        "    # Create a SummaryWriter instance for each run\n",
        "    writer = SummaryWriter(log_dir=f\"runs/gridCNN1406/{tag}\")\n",
        "\n",
        "    # Determine the number of flat features\n",
        "    # flat_feature = calculate_flat_features(input_shape=input_shape,\n",
        "    #                                        #conv1_filters=conv1_filter,\n",
        "    #                                        conv1_kernel_size=conv1_kernel,\n",
        "    #                                        #conv2_filters=conv2_filter,\n",
        "    #                                        conv2_kernel_size=conv2_kernel,\n",
        "    #                                        conv_stride=2,\n",
        "    #                                        use_second_conv_block=use_second)\n",
        "    # print(flat_feature)\n",
        "    # Create a new instance of the model for each combination of hyperparameters\n",
        "\n",
        "    print(\"Device:\", device)\n",
        "    model = to_device(CNNGridSearch(input_shape=input_shape,\n",
        "                                    conv1_filters=conv_filter,\n",
        "                                    conv1_kernel_size=conv_kernel,\n",
        "                                    maxpool_kernel_size=2,\n",
        "                                    conv2_filters=conv_filter,\n",
        "                                    conv2_kernel_size=conv_kernel,\n",
        "                                    conv_stride = 2,\n",
        "                                    use_second_conv_block=use_second,\n",
        "                                    dropout=0.1,\n",
        "                                    fc1_nodes=2048,\n",
        "                                    fc2_nodes=512,\n",
        "                                    fc3_nodes=128,\n",
        "                                    flat_features=9801), device)\n",
        "\n",
        "    # Train the model and evaluate its performance\n",
        "    history = [evaluate(model, val_dl)]\n",
        "    history += fit_one_cycle(epochs=epochs, max_lr=max_lr, model=model, train_loader=train_dl, val_loader=val_dl, opt_func=opt_func)\n",
        "\n",
        "    # Calculate the validation score\n",
        "    final_score = history[-1]['val_score']\n",
        "\n",
        "    # Check if the current combination is the best\n",
        "    if final_score > best_score:\n",
        "        print(\"Better parameters found, updating.\")\n",
        "        best_score = final_score\n",
        "        best_epochs = epochs\n",
        "        best_max_lr = max_lr\n",
        "\n",
        "    # Print the validation score for the current combination\n",
        "    print(f\"Epochs: {epochs}, Max LR: {max_lr}, Validation Score: {final_score}\")\n",
        "\n",
        "    # Close the writer for each run\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Epochs:\", best_epochs)\n",
        "print(\"Max LR:\", best_max_lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes|tensorboard dev upload --logdir /content/runs/gridCNN1406/ --name \"CNN() GRIDSearch Try\" --description \"Keeping dimensions the same for number of conv1 and conv2 channels\""
      ],
      "metadata": {
        "id": "CMFlMwqphtZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nzzsMcq2TCGm",
        "9uKOdXuoSjNy",
        "BUm2_vyHUWYC"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}